# Code

This directory contains the code for some of the experiments published in the paper.

## Experiments
The conducted experiments and their explanation.

#### saved_experiments/
Contains the data generated by the experiments. The sub-structure should be as follows:

* experiment_name
    * model.prm: saved neon network model
    * test-inference.pkl, test-targets.pkl, train-inference.pkl, train-targets.pkl: the predictions and targets for the generalist model.
    * spec_0/
        * test-inference.pkl, test-targets.pkl, train-inference.pkl, train-targets.pkl: the predictions and targets for the first specialist model.
    * spec_1/
        * test-inference.pkl, test-targets.pkl, train-inference.pkl, train-targets.pkl: the predictions and targets for the second specialist model.
    * ... for every following specialist.

#### model_layers.py
A python definition of some of the models trained.

#### fit_spec_err
Definition of a specialist experiment, to include in neon/neon/experiments/

#### specialist
A file to perform the clustering, and generate subset of datasets for specialists to learn. **It has to be copied in neon/neon/datasets** in order to be used. (and neon must be installed in development mode.)

#### 4_exp_cifar10_maxout_copy
A slightly modified version of the maxout paper on CIFAR10. (Uses ReLU, Dropout, and an additional layer) It reaches 20% error in 74 epochs.

#### 5_exp_cifar100_maxout_copy
An improved version of 4, on CIFAR100. It should be able to reach 45% test error in 740 epochs.

#### 8_exp_cifar10_specdataset
Training specialists on CIFAR10, based on 4.

#### 9_exp_cifar100_spec
Training specialists on CIFAR100, based on 5.

#### 10_hyperopt_exp
Experiement to optimize hyperparameters on 5 using spearmint.

#### 11_merge_predictions
Script allowing to merge the generalist and specialist predictions for a given experiment.

#### 12_cifar10_opt_exp
Experiment to optimize hyperparameters for CIFAR10 network 4.
19.61: epochs: 220, step_epochs: 3, weight_decay: 0.0001, lr: 0.08900302, momentum: 0.3990119

